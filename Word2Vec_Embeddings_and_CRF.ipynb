{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t27JlackriAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680555601384,
          "user_tz": -60,
          "elapsed": 2559,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "714ae31b-5a83-4d8c-f5e1-3c33215d2034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#mount to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25\n",
        "!pip install stop_words\n",
        "!pip install transformers\n",
        "!pip install sklearn_crfsuite"
      ],
      "metadata": {
        "id": "XnbeBf1fJ5os",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680555656066,
          "user_tz": -60,
          "elapsed": 54685,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "b35e6ad6-5f0c-4394-f8b2-b0d291efc499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.9/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from rank_bm25) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stop_words in /usr/local/lib/python3.9/dist-packages (2018.7.23)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.9/dist-packages (0.3.6)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite) (0.9.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite) (1.16.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite) (0.8.10)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.9/dist-packages (from sklearn_crfsuite) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIzeOgh9wYUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680556405019,
          "user_tz": -60,
          "elapsed": 2,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "b9b7e83e-478f-4a5d-bf70-f36a10ffe65e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pickle\n",
        "import scipy.stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib \n",
        "from itertools import chain\n",
        "from rank_bm25 import BM25Okapi\n",
        "from gensim.models import Word2Vec\n",
        "from stop_words import get_stop_words\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import sklearn_crfsuite\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJhkbzCBrnPU"
      },
      "outputs": [],
      "source": [
        "def preprocessing(content, remove_sw = False):\n",
        "\n",
        "    # convert the text to lowercase\n",
        "    content = content.lower() \n",
        "    regex = re.compile('[^a-z\\s]+')\n",
        "\n",
        "    # remove all commas so that constructions such as $70,000 maintain their meaning and do not get split:'70', '000'\n",
        "    content = regex.sub('', content)\n",
        "\n",
        "    # https://www.adamsmith.haus/python/answers/how-to-remove-all-punctuation-marks-with-nltk-in-python\n",
        "    # remove punctuation and tokenize (which will be the same as 1-grams)\n",
        "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "    one_grams = tokenizer.tokenize(content)\n",
        "\n",
        "    #remove stopwords\n",
        "    if remove_sw == True:\n",
        "        one_grams = [i for i in one_grams if i not in get_stop_words('english')]\n",
        "\n",
        "    # lemmatize\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = []\n",
        "    for word in one_grams:\n",
        "        words.append(lemmatizer.lemmatize(word))   \n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Data Processing**\n",
        "\n",
        "The unsupervised learning part was completed above, with the unsupervised data from kaggle (https://www.kaggle.com/datasets/jessicali9530/kuc-hackathon-winter-2018) and the combined supervised data sets from different sources. "
      ],
      "metadata": {
        "id": "BPLJuVOfJ8hM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5hF3GH1sF-Q"
      },
      "outputs": [],
      "source": [
        "# Download all data\n",
        "unlabeled_reviews_train = pd.read_csv('/content/drive/MyDrive/NLP Project/Data/Unsupervised drug reviews/drugsComTrain_raw.csv')\n",
        "unlabeled_reviews_test = pd.read_csv('/content/drive/MyDrive/NLP Project/Data/Unsupervised drug reviews/drugsComTest_raw.csv')\n",
        "\n",
        "labeled_drug_reviews = pd.read_csv(\"/content/drive/MyDrive/NLP Project/Data/Unsupervised drug reviews/Copy of combined_df_1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2n90w9gyy9O7"
      },
      "outputs": [],
      "source": [
        "# Concatenate unlabeled reviews\n",
        "unlabeled_drug_reviews = pd.concat([unlabeled_reviews_train, unlabeled_reviews_test], axis = 0)\n",
        "unlabeled_drug_reviews.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1680555661204,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          },
          "user_tz": -60
        },
        "id": "IfiXml-RzZE8",
        "outputId": "120f206b-cc82-4cd9-ae22-9f961d88a90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 216311 reviews.\n"
          ]
        }
      ],
      "source": [
        "#\u00a0Create lists of reviews for both datasets\n",
        "unlabeled_reviews_list = unlabeled_drug_reviews.review.to_list() #\u00a0A lists of lists. Contains characters\n",
        "labeled_reviews_list = labeled_drug_reviews.text.to_list()\n",
        "\n",
        "labeled_reviews_list = [x for x in labeled_reviews_list if str(x) != 'nan'] # Get rid of nans\n",
        "\n",
        "# Combine lists\n",
        "review_list = unlabeled_reviews_list\n",
        "review_list.extend(labeled_reviews_list)\n",
        "\n",
        "print(f\"There are {len(review_list)} reviews.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrF8Y9cy0see",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "executionInfo": {
          "status": "error",
          "timestamp": 1680548842080,
          "user_tz": -60,
          "elapsed": 133066,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "5c065006-de7c-4578-8d4a-e4927b55c227"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d27f3f9dca7e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenize reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreprocessed_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-d27f3f9dca7e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenize reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreprocessed_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-b3d386af8b67>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(content, remove_sw)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mone_grams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Tokenize reviews\n",
        "preprocessed_reviews = [preprocessing(i) for i in review_list]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "for review in preprocessed_reviews:\n",
        "  counter += len(review)\n",
        "\n",
        "print(counter)"
      ],
      "metadata": {
        "id": "tdF4SdL_hy4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Embeddings and clustering**\n",
        "\n",
        "1. Obtain embeddings using Word2Vec for the whole set of rewiews.\n",
        "2. Obtain clusters using K-means with 150 clusters, same number as in the paper. \n",
        "\n",
        "After, save the models."
      ],
      "metadata": {
        "id": "Z-jWeUmBKMLn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7pYzXA19IN-"
      },
      "outputs": [],
      "source": [
        "# Create Word embedding and clusters. Takes 3 minutes\n",
        "model = Word2Vec(sentences = preprocessed_reviews, vector_size= 150, min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load(\"/content/drive/MyDrive/NLP Project/Models/word2vec.model\")"
      ],
      "metadata": {
        "id": "-gyfSd_WhFhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frZ2FQhAO9Fb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680556130693,
          "user_tz": -60,
          "elapsed": 225094,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "9817701d-bd0a-4a54-9596-5c11564330fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Obtain the vector representations of the words. It's a dictionary\n",
        "word_vectors = model.wv\n",
        "\n",
        "vocab = np.array(list(model.wv.key_to_index.keys()))\n",
        "word_vecs = []\n",
        "\n",
        "for word in vocab:\n",
        "    word_vecs.append(word_vectors[word])\n",
        "    \n",
        "word_array = np.array(word_vecs)\n",
        "\n",
        "kmeans = KMeans(n_clusters=150).fit(word_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sxEGqYs9JyL"
      },
      "outputs": [],
      "source": [
        "# Save the vocab and the word arrays\n",
        "model.save(\"/content/drive/MyDrive/NLP Project/Models/word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQt_GhP9_FhO",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680556414008,
          "user_tz": -60,
          "elapsed": 6,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b9c297-908a-4814-a012-1bffc9289c12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/NLP Project/Models/model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# Save clustering \n",
        "joblib.dump(kmeans, \"/content/drive/MyDrive/NLP Project/Models/model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = joblib.load(\"/content/drive/MyDrive/NLP Project/Models/model.pkl\")"
      ],
      "metadata": {
        "id": "RsoqQWmj-JMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wk9hfgl4DqT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680556488100,
          "user_tz": -60,
          "elapsed": 432,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "7511a610-eda7-4693-e692-a39da9b5c7b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['stop', 'start', 'continue', 'become', 'treat', 'kick', 'avoid',\n",
              "       'begin', 'prevent', 'reduce', 'cure', 'return', 'ease', 'relieve',\n",
              "       'regulate', 'subside', 'improve', 'kill', 'trick', 'skip', 'lead',\n",
              "       'bring', 'discontinue', 'reach', 'heal', 'occur', 'develop',\n",
              "       'disappear', 'curb', 'lessen', 'eliminate', 'settle', 'alleviate',\n",
              "       'fade', 'worsen', 'resolve', 'suppress'], dtype='<U114')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# Investigate clusterings created\n",
        "label = 17\n",
        "mask = (kmeans.labels_ == label)\n",
        "vocab[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1w9mqPjAhGT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1679662422613,
          "user_tz": 0,
          "elapsed": 194,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "f7eeb058-e08f-49b2-b9e3-acc57a754a25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['pill', 'control', 'birth', 'patch', 'bc', 'generic', 'brand',\n",
              "       'depo', 'yaz', 'nuvaring', 'loestrin', 'fe', 'lo', 'hormonal',\n",
              "       'sprintec', 'ortho', 'ring', 'placebo', 'yasmin', 'trinessa',\n",
              "       'contraceptive', 'tri', 'lutera', 'aviane', 'microgestin', 'apri',\n",
              "       'junel', 'tricyclen', 'seasonique', 'trisprintec', 'beyaz',\n",
              "       'cyclen', 'gianvi', 'alesse', 'contraception', 'nuva', 'ocella',\n",
              "       'levora', 'minastrin', 'gildess', 'blisovi', 'evra', 'generess'],\n",
              "      dtype='<U114')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "label = 17\n",
        "mask = (kmeans.labels_ == label)\n",
        "vocab[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GLkGdbOAptR",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1679662426401,
          "user_tz": 0,
          "elapsed": 190,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "8b2bbf97-bd6c-4bf4-ea58-14ee722ca921"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['sleep', 'function', 'walk', 'move', 'breath', 'sit', 'breathe'],\n",
              "      dtype='<U114')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "label = 40\n",
        "mask = (kmeans.labels_ == label)\n",
        "vocab[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSnWhvw7A1KM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1679662429089,
          "user_tz": 0,
          "elapsed": 194,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "dbfc1272-57fc-4242-d053-93e6de4a373f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['up', 'out', 'back', 'off', 'down', 'away', 'without', 'through',\n",
              "       'around', 'into', 'bed', 'point', 'home', 'bathroom', 'rest',\n",
              "       'toilet', 'outside', 'store', 'anywhere', 'thru', 'urgent',\n",
              "       'ahead', 'restroom', 'downhill'], dtype='<U114')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "label = 109\n",
        "mask = (kmeans.labels_ == label)\n",
        "vocab[mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha_TfmNzDgy4"
      },
      "source": [
        "##**CRF with Word2Vec and K-means**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/NLP Project/Code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cOD04ixKp2r",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680556515818,
          "user_tz": -60,
          "elapsed": 417,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "26f5e71f-100d-4c10-c833-fdec4f5628b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/NLP Project/Code'\n",
            "/content/drive/MyDrive/NLP Project/Code\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import custom functions\n",
        "from bert_text_pre_processing import add_labels\n",
        "from CRF_utils import sent2features"
      ],
      "metadata": {
        "id": "9dduUk1ZKsH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\u00a0Load word2Vec and kmeans\n",
        "model = Word2Vec.load(\"/content/drive/MyDrive/NLP Project/Models/word2vec.model\")\n",
        "kmeans = joblib.load(\"/content/drive/MyDrive/NLP Project/Models/model.pkl\")\n",
        "\n",
        "word_vectors = model.wv\n",
        "\n",
        "vocab = np.array(list(model.wv.key_to_index.keys()))\n",
        "word_vecs = []\n",
        "\n",
        "for word in vocab:\n",
        "    word_vecs.append(word_vectors[word])\n",
        "    \n",
        "word_array = np.array(word_vecs)"
      ],
      "metadata": {
        "id": "RAJoydh6LGcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Data processing for CRF**\n",
        "\n",
        "For each token in each review, create dictionary containing:\n",
        "1. The three previous and following tokens. \n",
        "2. The respective clusters of the aforementioned tokens. "
      ],
      "metadata": {
        "id": "_aNIJZsFLLCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = pd.read_csv(r'/content/drive/MyDrive/NLP Project/Data/Combined Datasets/combined_df_1.csv')\n",
        "df_2 = pd.read_csv(r'/content/drive/MyDrive/NLP Project/Data/Combined Datasets/combined_df_2.csv')\n",
        "\n",
        "pre_processed = add_labels(df_1, df_2, 'other', 'text', 'txt_id', 'symptom', False)\n",
        "\n",
        "# split dataset into training and test/val\n",
        "np.random.seed(100)\n",
        "\n",
        "train_df, not_train_df = train_test_split(pre_processed, test_size=0.2)\n",
        "valid_df, test_df = train_test_split(not_train_df, test_size=0.5)\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "YOG9b-SvLJJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['text'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "TTnwjLkL9jGI",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680556330200,
          "user_tz": -60,
          "elapsed": 8,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "6b2a057e-8841-4b38-d705-40f04cea07b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Within 5 minutes of taking drug, developed severe colon and uterine cramping. Cold sweat, fainting, heart palpitations. Will never use again or recommend. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of sentences for each DataFrame. Remove sentences with less than \n",
        "# 6 tokens and make sure the labels are strings. \n",
        "\n",
        "train_sentences, y_train_CRF = [], []\n",
        "val_sentences, y_val_CRF = [], []\n",
        "test_sentences, y_test_CRF = [], []\n",
        "\n",
        "for sent, lab in zip(train_df.tokenized.to_list(), train_df.pre_processed_tokens.to_list()):\n",
        "  if len(sent) >= 6 and len(lab) == len(sent):\n",
        "    train_sentences.append(sent)\n",
        "    y_train_CRF.append(np.array(lab, dtype = 'str').tolist())\n",
        "\n",
        "for sent, lab in zip(valid_df.tokenized.to_list(), valid_df.pre_processed_tokens.to_list()):\n",
        "  if len(sent) >= 6 and len(lab) == len(sent):\n",
        "    val_sentences.append(sent)\n",
        "    y_val_CRF.append(np.array(lab, dtype = 'str').tolist())\n",
        "\n",
        "for sent, lab in zip(test_df.tokenized.to_list(), test_df.pre_processed_tokens.to_list()):\n",
        "  if len(sent) >= 6 and len(lab) == len(sent):\n",
        "    test_sentences.append(sent)\n",
        "    y_test_CRF.append(np.array(lab, dtype = 'str').tolist())"
      ],
      "metadata": {
        "id": "mYaoWJllLmPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_counter = 0\n",
        "val_counter = 0\n",
        "test_counter = 0\n",
        "\n",
        "for sent in train_sentences:\n",
        "  train_counter += len(sent)\n",
        "\n",
        "for sent in val_sentences:\n",
        "  val_counter += len(sent)\n",
        "\n",
        "for sent in test_sentences:\n",
        "  test_counter += len(sent)\n",
        "\n",
        "print(f\"Train: {len(train_sentences)}, tokens - {train_counter}\")\n",
        "print(f\"Val: {len(val_sentences)}, tokens - {val_counter}\")\n",
        "print(f\"Test: {len(test_sentences)}, tokens - {test_counter}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAHTiJLrlFNt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680449302071,
          "user_tz": -60,
          "elapsed": 5,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "9caf8650-569c-453b-c2bb-9b9f471b5467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1606, tokens - 142135\n",
            "Val: 200, tokens - 18467\n",
            "Test: 206, tokens - 20658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_sentences), len(y_train_CRF))\n",
        "print(len(val_sentences), len(y_val_CRF))\n",
        "print(len(test_sentences), len(y_test_CRF))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um_vobxAa29X",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680362804503,
          "user_tz": -60,
          "elapsed": 4,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "1ecad6dc-22f4-4bae-e722-1f85cb80299f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1606 1606\n",
            "200 200\n",
            "206 206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\u00a0Get CRF features for the three sets\n",
        "X_train = [sent2features(s, vocab,  kmeans.labels_) for s in train_sentences]\n",
        "X_val = [sent2features(s, vocab,  kmeans.labels_) for s in val_sentences]\n",
        "X_test = [sent2features(s, vocab,  kmeans.labels_) for s in test_sentences]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "0xQ5AQGhMLlr",
        "executionInfo": {
          "status": "error",
          "timestamp": 1680363149956,
          "user_tz": -60,
          "elapsed": 318069,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "5e212f68-8c29-4c1c-eec0-485a9928e727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-864c3bede32e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#\u00a0Get CRF features for the three sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#X_train = [sent2features(s, vocab,  kmeans.labels_) for s in train_sentences]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#X_test = [sent2features(s, vocab,  kmeans.labels_) for s in test_sentences]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-864c3bede32e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#\u00a0Get CRF features for the three sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#X_train = [sent2features(s, vocab,  kmeans.labels_) for s in train_sentences]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msent2features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#X_test = [sent2features(s, vocab,  kmeans.labels_) for s in test_sentences]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/NLP Project/Code/CRF_utils.py\u001b[0m in \u001b[0;36msent2features\u001b[0;34m(sent, vocab, cluster_ids)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Get all the features of each word in a sentence in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msent2features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/NLP Project/Code/CRF_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Get all the features of each word in a sentence in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msent2features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword2features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/NLP Project/Code/CRF_utils.py\u001b[0m in \u001b[0;36mword2features\u001b[0;34m(sent, i, vocab, cluster_ids)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Start by initializing position independent features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     features = {\"word\": word, \n\u001b[0;32m---> 19\u001b[0;31m                 \"wordC\": find_cluster(word, vocab, cluster_ids)}\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/NLP Project/Code/CRF_utils.py\u001b[0m in \u001b[0;36mfind_cluster\u001b[0;34m(word, vocab, cluster_ids)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mword_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcluster_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\u00a0Save files\n",
        "with open(\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/xTrain\", \"wb\") as f:\n",
        "    pickle.dump(X_train, f)\n",
        "with open(\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/xVal\", \"wb\") as f:\n",
        "    pickle.dump(X_val, f)\n",
        "with open(\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/xTest\", \"wb\") as f:\n",
        "    pickle.dump(X_test, f)\n",
        "with open(\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/yTrain\", \"wb\") as f:\n",
        "    pickle.dump(y_train_CRF, f)\n",
        "with open(\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/yVal\", \"wb\") as f:\n",
        "    pickle.dump(y_val_CRF, f)\n",
        "with open(\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/yTest\", \"wb\") as f:\n",
        "    pickle.dump(y_test_CRF, f)"
      ],
      "metadata": {
        "id": "o3g5kTKjMVtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load files\n",
        "X_train = pd.read_pickle(r\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/xTrain\")\n",
        "X_val = pd.read_pickle(r\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/xVal\")\n",
        "X_test = pd.read_pickle(r\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/xTest\")\n",
        "\n",
        "y_train_CRF = pd.read_pickle(r\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/yTrain\")\n",
        "y_val_CRF = pd.read_pickle(r\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/yVal\")\n",
        "y_test_CRF = pd.read_pickle(r\"/content/drive/MyDrive/NLP Project/Data/CRF_Data/yTest\")\n"
      ],
      "metadata": {
        "id": "xzeN53ZBMc6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Hyperparameter tuning: number of iterations for CRF**"
      ],
      "metadata": {
        "id": "SuYqDao-MlbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "no_iters = [50, 60, 70, 80, 90, 100]\n",
        "\n",
        "for n in no_iters:\n",
        "\n",
        "  crf = sklearn_crfsuite.CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=n, all_possible_transitions=True)\n",
        "  crf.fit(X_train, y_train_CRF)\n",
        "  \n",
        "  y_pred_train = crf.predict(X_train)\n",
        "  y_pred = crf.predict(X_val[:200])\n",
        "\n",
        "  F1_val = metrics.flat_f1_score(y_val_CRF, y_pred, pos_label='1')\n",
        "  F1_train= metrics.flat_f1_score(y_train_CRF, y_pred_train, pos_label='1')\n",
        "\n",
        "  print(f\"For {n} iterations, the train and validation F1-scores are {F1_train:.2f} and {F1_val:.2f}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "IXLCHUNKMsGv",
        "executionInfo": {
          "status": "error",
          "timestamp": 1680363207250,
          "user_tz": -60,
          "elapsed": 16112,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "1c8fd6d4-8b15-431b-f0ce-11babff85d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-2f9d749ede99>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mF1_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_f1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_CRF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mF1_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_f1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_CRF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn_crfsuite/metrics.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(y_true, y_pred, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_true_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0my_pred_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn_crfsuite/metrics.py\u001b[0m in \u001b[0;36mflat_f1_score\u001b[0;34m(y_true, y_pred, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \"\"\"\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.66666667\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m        \u001b[0;34m,\u001b[0m \u001b[0;36m0.66666667\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \"\"\"\n\u001b[0;32m-> 1146\u001b[0;31m     return fbeta_score(\n\u001b[0m\u001b[1;32m   1147\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     \"\"\"\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m     _, _, f, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   1288\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average has to be one of \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [18467, 20159]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_no_iters = 70"
      ],
      "metadata": {
        "id": "WPxYOMEJM2Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Model. Final results**"
      ],
      "metadata": {
        "id": "xAVitjxuM8Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\u00a0Train CRF\n",
        "crf = sklearn_crfsuite.CRF(algorithm='lbfgs',c1=0.1, c2=0.1, max_iterations=100, all_possible_transitions=True)\n",
        "crf.fit(X_train, y_train_CRF)\n",
        "\n",
        "#\u00a0Print results\n",
        "y_pred = crf.predict(X_test)\n",
        "\n",
        "acc = metrics.flat_accuracy_score(y_test_CRF, y_pred)\n",
        "recall = metrics.flat_recall_score(y_test_CRF, y_pred, pos_label='1')\n",
        "precision = metrics.flat_precision_score(y_test_CRF, y_pred, pos_label='1')\n",
        "F1 = metrics.flat_f1_score(y_test_CRF, y_pred, pos_label='1')\n",
        "\n",
        "print(f\"Accuracy - {acc}, Recall - {recall}, Precision - {precision}, F1 - {F1}.\")"
      ],
      "metadata": {
        "id": "EQrtwteMM_oK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680556577075,
          "user_tz": -60,
          "elapsed": 38077,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "6f2fe99d-d1cf-47ed-8931-9f2ab285d840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy - 0.9018297995933778, Recall - 0.5269102990033223, Precision - 0.7242009132420091, F1 - 0.61.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(crf, \"/content/drive/MyDrive/NLP Project/Models/CRF_word2vec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnIAHX18-cX3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680556591294,
          "user_tz": -60,
          "elapsed": 428,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "1ce0ee81-a6b0-4b03-f336-2e460a796822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/NLP Project/Models/CRF_word2vec']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsrqpSPB9TJ1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1680556578284,
          "user_tz": -60,
          "elapsed": 6,
          "user": {
            "displayName": "Marta Emili",
            "userId": "12355713369317347230"
          }
        },
        "outputId": "d0ab5425-7990-491a-e81a-7b3ad22414ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0',\n",
              " '0',\n",
              " '0',\n",
              " '0',\n",
              " '0',\n",
              " '0',\n",
              " '0',\n",
              " '0',\n",
              " '0',\n",
              " '0',\n",
              " '1',\n",
              " '1',\n",
              " '1',\n",
              " '1',\n",
              " '1',\n",
              " '1',\n",
              " '0',\n",
              " '0',\n",
              " '0',\n",
              " '0',\n",
              " '0',\n",
              " '0']"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}