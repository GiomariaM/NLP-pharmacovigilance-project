{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSeCRzs//JkoUtsHIe/Nqy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOz1OtPScW7D","executionInfo":{"status":"ok","timestamp":1747747442507,"user_tz":-60,"elapsed":29168,"user":{"displayName":"Karina","userId":"10900104624874413001"}},"outputId":"2d4297cd-bea6-409d-81e2-606dc2a70ed6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rank_bm25\n","  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (2.0.2)\n","Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n","Installing collected packages: rank_bm25\n","Successfully installed rank_bm25-0.2.2\n","Collecting stop_words\n","  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: stop_words\n","  Building wheel for stop_words (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stop_words: filename=stop_words-2018.7.23-py3-none-any.whl size=32894 sha256=0c9de0a0f9435d23c460dc12d66782eb9fd1010af28eb86559294f89d307b7c5\n","  Stored in directory: /root/.cache/pip/wheels/8f/a5/51/a5405e1da5d178491b79d12cc81b6cb9bb14fe2c8c632eba70\n","Successfully built stop_words\n","Installing collected packages: stop_words\n","Successfully installed stop_words-2018.7.23\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install rank_bm25\n","!pip install stop_words\n","\n","import pandas as pd\n","import nltk\n","import re\n","from stop_words import get_stop_words\n","from nltk.stem import WordNetLemmatizer\n","from rank_bm25 import BM25Okapi\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","nltk.download('wordnet')"]},{"cell_type":"code","source":["# get ADR lexicon dataframe\n","adr_lexicon = pd.read_csv('ADR_lexicon.txt', sep='\\t', names=['id', 'reaction', 'source'])\n","lexicon_list = adr_lexicon.reaction.to_list()"],"metadata":{"id":"RjCVZt4lcqyW","executionInfo":{"status":"error","timestamp":1747747447608,"user_tz":-60,"elapsed":423,"user":{"displayName":"Karina","userId":"10900104624874413001"}},"colab":{"base_uri":"https://localhost:8080/","height":316},"outputId":"dcc9c966-5a80-4c9f-fa02-158da9734137"},"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'ADR_lexicon.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-bd0aa2439048>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get ADR lexicon dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0madr_lexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ADR_lexicon.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reaction'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlexicon_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madr_lexicon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ADR_lexicon.txt'"]}]},{"cell_type":"code","source":["# ger reviews daatframe\n","reviews_df = pd.read_csv('combined_df_1.csv')\n","reviews_list = reviews_df.text.to_list()\n","reviews_id = reviews_df.txt_id.to_list()\n","\n","# 2388 reviews in total"],"metadata":{"id":"fKUrvSUNc23i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get list of nan reviews\n","d = dict(zip(reviews_id,reviews_list))\n","list_nan = [key for key, value in d.items() if isinstance(value, float)]"],"metadata":{"id":"anztH9fUc6Rd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove nan reviews from review dataframe\n","reviews_df = reviews_df[~reviews_df['txt_id'].isin(list_nan)]\n","reviews_list = reviews_df.text.to_list()\n","reviews_id = reviews_df.txt_id.to_list()\n","\n","# 2254 after removing nan"],"metadata":{"id":"EHqBo_JKc9nC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove nan reviews from adr dataframe\n","adr_df = pd.read_csv('combined_df_2.csv')\n","adr_df = adr_df[~adr_df['txt_id'].isin(list_nan)]"],"metadata":{"id":"S0slKD5bdAo2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build initial ADR dictionary\n","ADRs = {}\n","for i in reviews_id:\n","    ADRs[i] = adr_df.loc[adr_df['txt_id'] == i]['symptom'].to_list()"],"metadata":{"id":"hNo0Q2qedDoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get review IDs that are in review dataframe but not in adr dataframe\n","no_adr_reviews = [k for k, v in ADRs.items() if v in (None, \"\", [])]"],"metadata":{"id":"AU5AnXIldHJx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# remove them\n","reviews_df = reviews_df[~reviews_df['txt_id'].isin(no_adr_reviews)]\n","reviews_list = reviews_df.text.to_list()\n","reviews_id = reviews_df.txt_id.to_list()\n","\n","# 2058 after removing the not annotated ones\n","\n","adr_df = adr_df[~adr_df['txt_id'].isin(no_adr_reviews)]"],"metadata":{"id":"cAjyX1A2dJMw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# build final ADR dictionary\n","ADRs = {}\n","for i in reviews_id:\n","    ADRs[i] = adr_df.loc[adr_df['txt_id'] == i]['symptom'].to_list()"],"metadata":{"id":"4EUzJJFLdM_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocessing(content, remove_sw):\n","    # convert the text to lowercase\n","    content = content.lower()\n","\n","    # remove non-alphabetical characters\n","    regex = re.compile('[^a-z\\s]+')\n","    content = regex.sub('', content)\n","\n","    # https://www.adamsmith.haus/python/answers/how-to-remove-all-punctuation-marks-with-nltk-in-python\n","    # remove punctuation and tokenize (which will be the same as 1-grams)\n","    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n","    one_grams = tokenizer.tokenize(content)\n","\n","    #remove stopwords\n","    if remove_sw == True:\n","        one_grams = [i for i in one_grams if i not in get_stop_words('english')]\n","\n","    # lemmatize\n","    lemmatizer = WordNetLemmatizer()\n","    words = []\n","    for word in one_grams:\n","        words.append(lemmatizer.lemmatize(word))\n","\n","    return words"],"metadata":{"id":"b39zQt45dVAN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_scores(model, lexicon_list=lexicon_list, reviews_list=reviews_list,  threshold_bm25=0.78, threshold_tf_idf=0.32):\n","\n","    preprocessed_ADRs = [preprocessing(i, remove_sw=True) for i in lexicon_list]\n","    preprocessed_reviews = [preprocessing(i, remove_sw=True) for i in reviews_list]\n","    total_no_adrs = len(lexicon_list)\n","\n","    # choose model\n","    if model == 'bm25':\n","        bm25 = BM25Okapi(preprocessed_ADRs)\n","\n","        precision_list = []\n","        recall_list = []\n","        f1_score_list = []\n","        accuracy_list = []\n","        no_retrieved_adrs = []\n","\n","        for i, j in zip(range(len(preprocessed_reviews)), reviews_id):\n","\n","            # get actual ADRs\n","            actual_ADRs = ADRs[j]\n","\n","            #get the scores for every ADRs for this specific review\n","            score_list = bm25.get_scores(preprocessed_reviews[i])\n","\n","            # build dataframe with scores\n","            scores_df = pd.DataFrame({'ADR': lexicon_list, 'score': score_list})\n","\n","            # remove rows with score=0\n","            scores_df = scores_df[scores_df['score'] != 0]\n","\n","            # normalize scores\n","            scores_df['normalized_score'] = (scores_df['score'] - scores_df.score.min()) / (scores_df.score.max() - scores_df.score.min())\n","\n","            # keep only scores over thershold\n","            scores_df = scores_df[scores_df['normalized_score'] > threshold_bm25]\n","            no_retrieved_adrs.append(len(scores_df))\n","\n","            # get final list of ADRs obtained from model\n","            model_ADRs = scores_df.ADR.to_list()\n","\n","            # compute metrics\n","            TP = 0\n","            FP = 0\n","            for i in model_ADRs:\n","                if i in actual_ADRs: TP += 1\n","                else: FP += 1\n","\n","            FN = len(actual_ADRs) - TP\n","\n","            no_actual = [i for i in lexicon_list if i not in actual_ADRs]\n","            TN_list = [i for i in no_actual if i not in model_ADRs]\n","            TN = len(TN_list)\n","\n","            recall = TP / (TP + FN)\n","\n","            if TP + FP != 0: precision = TP / (TP + FP)\n","            else: precision = 0\n","\n","            if TP != 0: f1_score = 2 * ((precision * recall) / (precision + recall))\n","            else: f1_score = 0\n","\n","            accuracy = (TP + TN) / total_no_adrs\n","\n","            precision_list.append(precision)\n","            recall_list.append(recall)\n","            f1_score_list.append(f1_score)\n","            accuracy_list.append(accuracy)\n","\n","        # buid results dataframe\n","        results_df = pd.DataFrame({'review_id': reviews_id, 'precision': precision_list, 'recall': recall_list,\n","                                  'f1_score': f1_score_list, 'accuracy': accuracy_list})\n","\n","        precision = sum(results_df.precision.to_list()) / len(results_df.precision.to_list())\n","        recall = sum(results_df.recall.to_list()) / len(results_df.recall.to_list())\n","        f1 = sum(results_df.f1_score.to_list()) / len(results_df.f1_score.to_list())\n","        accuracy = sum(results_df.accuracy.to_list()) / len(results_df.accuracy.to_list())\n","        print('Number of ADRs retrieved for each review', no_retrieved_adrs)\n","        print('Average number of ADRs retrieved:', sum(no_retrieved_adrs)/len(no_retrieved_adrs))\n","\n","        # return results\n","        return 'BM25 results: ', 'precision:', precision, 'recall:', recall, 'f1-score:', f1, 'accuracy', accuracy\n","\n","    elif model == 'tf_idf':\n","        vectorizer = TfidfVectorizer()\n","        tfidf_matrix = vectorizer.fit_transform(lexicon_list)\n","\n","        precision_list = []\n","        recall_list = []\n","        f1_score_list = []\n","        accuracy_list = []\n","        no_retrieved_adrs = []\n","\n","        for i, j in zip(range(len(reviews_list)), reviews_id):\n","\n","            # get actual ADRs\n","            actual_ADRs = ADRs[j]\n","\n","            query_tfidf = vectorizer.transform([reviews_list[i]])\n","\n","            # get scores\n","            cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n","\n","            # build dataframe with scores\n","            scores_df = pd.DataFrame({'ADR': lexicon_list, 'score': cosine_similarities})\n","\n","            # keep only scores over thershold\n","            scores_df = scores_df[scores_df['score'] > threshold_tf_idf]\n","            no_retrieved_adrs.append(len(scores_df))\n","\n","            # get final list of ADRs obtained from model\n","            model_ADRs = scores_df.ADR.to_list()\n","\n","            # compute metrics\n","            TP = 0\n","            FP = 0\n","            for i in model_ADRs:\n","                if i in actual_ADRs: TP += 1\n","                else: FP += 1\n","\n","            FN = len(actual_ADRs) - TP\n","\n","            no_actual = [i for i in lexicon_list if i not in actual_ADRs]\n","            TN_list = [i for i in no_actual if i not in model_ADRs]\n","            TN = len(TN_list)\n","\n","            recall = TP / (TP + FN)\n","\n","            if TP + FP != 0: precision = TP / (TP + FP)\n","            else: precision = 0\n","\n","            if TP != 0: f1_score = 2 * ((precision * recall) / (precision + recall))\n","            else: f1_score = 0\n","\n","            accuracy = (TP + TN) / total_no_adrs\n","\n","            precision_list.append(precision)\n","            recall_list.append(recall)\n","            f1_score_list.append(f1_score)\n","            accuracy_list.append(accuracy)\n","\n","        # buid results dataframe\n","        results_df = pd.DataFrame({'review_id': reviews_id, 'precision': precision_list, 'recall': recall_list,\n","                                  'f1_score': f1_score_list, 'accuracy': accuracy_list})\n","\n","        precision = sum(results_df.precision.to_list()) / len(results_df.precision.to_list())\n","        recall = sum(results_df.recall.to_list()) / len(results_df.recall.to_list())\n","        f1 = sum(results_df.f1_score.to_list()) / len(results_df.f1_score.to_list())\n","        accuracy = sum(results_df.accuracy.to_list()) / len(results_df.accuracy.to_list())\n","        print('Number of ADRs retrieved for each review', no_retrieved_adrs)\n","        print('Average number of ADRs retrieved:', sum(no_retrieved_adrs)/len(no_retrieved_adrs))\n","\n","        # return results\n","        return 'TF-IDF results: ', 'precision:', precision, 'recall:', recall, 'f1-score:', f1, 'accuracy', accuracy"],"metadata":{"id":"OoxhclTXdZOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(get_scores('tf_idf'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5xKvU8ldtHi","executionInfo":{"status":"ok","timestamp":1680531763771,"user_tz":-60,"elapsed":18894,"user":{"displayName":"Karina","userId":"10900104624874413001"}},"outputId":"67e1239d-753c-48ad-ef74-4611e4354fa2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of ADRs retrieved for each review [3, 1, 1, 13, 3, 6, 7, 2, 5, 215, 0, 2, 3, 2, 7, 5, 1, 26, 1, 19, 8, 0, 10, 3, 3, 14, 15, 1, 18, 3, 2, 1, 3, 40, 0, 1, 1, 4, 7, 29, 0, 15, 24, 16, 26, 1, 5, 0, 0, 3, 38, 1, 6, 4, 2, 0, 1, 0, 2, 0, 2, 25, 11, 0, 6, 0, 0, 0, 0, 0, 36, 1, 0, 3, 2, 5, 6, 0, 22, 18, 4, 1, 2, 15, 0, 0, 20, 3, 0, 1, 8, 5, 4, 1, 2, 2, 12, 0, 21, 4, 3, 2, 5, 14, 0, 2, 2, 0, 3, 2, 2, 1, 0, 2, 0, 8, 1, 2, 10, 3, 2, 0, 1, 3, 14, 1, 4, 2, 1, 2, 2, 6, 3, 7, 0, 0, 0, 0, 17, 0, 6, 0, 19, 2, 0, 17, 3, 1, 6, 6, 0, 1, 0, 2, 2, 1, 29, 3, 18, 0, 13, 1, 0, 69, 2, 1, 4, 5, 1, 0, 1, 54, 1, 5, 2, 13, 4, 0, 5, 2, 4, 1, 2, 5, 0, 3, 4, 0, 26, 13, 1, 19, 5, 0, 2, 2, 2, 12, 3, 0, 37, 6, 4, 3, 8, 2, 1, 7, 11, 2, 0, 10, 1, 28, 1, 2, 2, 0, 0, 0, 0, 15, 3, 0, 2, 3, 0, 15, 2, 4, 0, 5, 19, 12, 0, 4, 4, 0, 1, 1, 3, 3, 0, 0, 13, 2, 35, 3, 0, 5, 14, 0, 3, 0, 2, 14, 13, 0, 1, 1, 2, 2, 3, 2, 0, 27, 1, 2, 7, 13, 2, 0, 1, 0, 3, 4, 2, 3, 1, 19, 1, 3, 2, 0, 17, 1, 21, 3, 2, 6, 0, 16, 2, 2, 26, 2, 5, 2, 0, 4, 0, 3, 4, 0, 2, 1, 2, 0, 1, 2, 12, 1, 1, 9, 2, 3, 0, 0, 3, 0, 4, 2, 8, 0, 4, 1, 1, 0, 0, 13, 2, 2, 1, 2, 2, 5, 1, 0, 5, 3, 1, 2, 0, 7, 3, 4, 3, 37, 3, 0, 1, 2, 2, 9, 3, 0, 3, 0, 2, 3, 1, 2, 0, 20, 0, 4, 3, 4, 0, 0, 4, 33, 2, 3, 0, 27, 1, 2, 20, 0, 10, 0, 0, 16, 3, 0, 0, 1, 2, 1, 1, 6, 1, 0, 5, 4, 3, 0, 0, 1, 20, 3, 3, 2, 3, 1, 1, 45, 8, 3, 5, 3, 2, 3, 0, 1, 1, 0, 17, 2, 4, 2, 1, 4, 1, 20, 1, 1, 0, 1, 11, 1, 4, 0, 3, 0, 13, 2, 8, 1, 3, 0, 1, 1, 5, 1, 6, 2, 1, 13, 1, 4, 1, 2, 3, 0, 4, 3, 9, 6, 2, 0, 5, 4, 0, 0, 1, 13, 2, 2, 2, 11, 3, 8, 13, 13, 0, 2, 2, 10, 0, 2, 2, 0, 1, 4, 2, 2, 7, 3, 1, 8, 1, 5, 3, 9, 0, 2, 0, 2, 2, 29, 2, 2, 1, 3, 4, 0, 0, 28, 1, 1, 1, 6, 3, 2, 3, 3, 0, 26, 2, 13, 3, 14, 3, 20, 45, 1, 20, 2, 1, 2, 0, 5, 6, 1, 5, 7, 2, 2, 1, 17, 1, 1, 0, 3, 3, 0, 0, 0, 8, 2, 1, 0, 2, 11, 3, 2, 7, 3, 0, 17, 3, 2, 3, 0, 7, 2, 2, 1, 2, 10, 2, 1, 2, 0, 3, 2, 22, 2, 7, 1, 1, 1, 6, 5, 3, 2, 2, 9, 6, 0, 15, 0, 16, 1, 1, 0, 1, 0, 9, 13, 2, 2, 2, 4, 1, 1, 1, 0, 1, 14, 0, 0, 2, 4, 14, 2, 3, 18, 2, 16, 1, 1, 2, 1, 1, 2, 0, 3, 7, 4, 1, 0, 3, 0, 5, 4, 2, 3, 1, 0, 4, 2, 3, 2, 4, 1, 0, 0, 2, 1, 0, 4, 0, 19, 3, 1, 0, 2, 5, 38, 0, 2, 18, 0, 1, 1, 1, 0, 1, 0, 5, 14, 1, 1, 3, 0, 2, 3, 4, 8, 2, 0, 4, 1, 2, 2, 3, 2, 7, 12, 1, 4, 8, 4, 7, 3, 10, 1, 2, 2, 13, 5, 13, 2, 0, 6, 4, 1, 7, 9, 27, 2, 2, 4, 1, 8, 7, 2, 0, 0, 0, 1, 20, 10, 0, 1, 5, 7, 6, 1, 4, 1, 1, 0, 6, 8, 3, 1, 4, 15, 4, 10, 3, 6, 3, 3, 2, 9, 8, 0, 1, 0, 1, 8, 12, 2, 4, 0, 20, 0, 3, 0, 1, 10, 0, 3, 14, 2, 2, 3, 1, 1, 22, 0, 1, 3, 2, 47, 0, 1, 1, 0, 1, 2, 4, 0, 0, 3, 6, 6, 7, 2, 4, 2, 8, 1, 1, 5, 4, 5, 4, 0, 0, 2, 30, 11, 0, 0, 7, 0, 1, 9, 13, 1, 16, 0, 8, 46, 4, 4, 15, 2, 2, 2, 0, 12, 41, 2, 2, 1, 0, 2, 2, 3, 1, 3, 6, 1, 1, 1, 17, 7, 0, 0, 2, 22, 0, 0, 3, 4, 62, 0, 1, 7, 1, 1, 1, 0, 3, 1, 23, 0, 24, 1, 1, 1, 0, 2, 0, 10, 1, 5, 13, 3, 2, 2, 3, 62, 0, 7, 0, 0, 1, 2, 4, 0, 0, 1, 1, 1, 2, 3, 2, 4, 2, 3, 40, 6, 1, 2, 17, 27, 1, 13, 7, 2, 0, 22, 1, 7, 0, 3, 3, 2, 1, 1, 0, 1, 1, 2, 2, 3, 0, 1, 9, 11, 1, 1, 2, 1, 6, 2, 4, 2, 10, 10, 1, 2, 1, 0, 15, 2, 1, 9, 0, 0, 15, 1, 0, 11, 5, 4, 7, 14, 0, 2, 11, 3, 1, 5, 1, 1, 13, 1, 4, 3, 14, 2, 9, 2, 1, 3, 8, 1, 1, 6, 3, 3, 7, 13, 18, 3, 3, 1, 2, 3, 0, 17, 0, 1, 5, 1, 7, 0, 0, 0, 1, 6, 11, 4, 11, 1, 0, 1, 0, 1, 5, 1, 115, 0, 0, 0, 11, 1, 6, 0, 3, 7, 1, 37, 4, 3, 2, 1, 2, 2, 2, 1, 0, 3, 0, 6, 2, 0, 51, 2, 7, 46, 0, 3, 8, 2, 16, 3, 4, 1, 1, 2, 1, 5, 0, 8, 1, 2, 0, 2, 3, 0, 3, 3, 13, 4, 0, 2, 5, 46, 5, 2, 11, 215, 8, 0, 40, 1, 2, 1, 1, 0, 0, 4, 1, 0, 3, 3, 15, 3, 2, 3, 15, 3, 3, 0, 3, 4, 2, 8, 7, 26, 2, 2, 2, 15, 0, 4, 1, 0, 0, 5, 0, 11, 7, 4, 15, 2, 4, 11, 1, 3, 0, 1, 7, 39, 7, 2, 5, 2, 2, 2, 0, 3, 4, 4, 4, 4, 1, 0, 1, 0, 5, 0, 6, 0, 1, 2, 0, 0, 2, 2, 0, 11, 3, 2, 3, 3, 0, 6, 4, 1, 1, 2, 11, 1, 19, 2, 2, 0, 1, 3, 0, 1, 1, 3, 2, 6, 6, 3, 2, 0, 0, 3, 0, 4, 1, 4, 4, 2, 0, 0, 7, 1, 3, 2, 3, 5, 13, 0, 0, 0, 0, 2, 5, 1, 2, 5, 8, 0, 0, 1, 1, 1, 2, 1, 1, 4, 2, 3, 3, 0, 0, 1, 0, 6, 3, 3, 1, 0, 3, 2, 0, 1, 0, 0, 3, 2, 0, 2, 2, 2, 1, 0, 1, 10, 1, 2, 1, 2, 1, 1, 0, 1, 4, 0, 1, 0, 12, 0, 4, 0, 4, 1, 12, 2, 0, 0, 3, 8, 0, 0, 4, 2, 1, 1, 1, 1, 7, 3, 3, 0, 0, 1, 3, 3, 0, 0, 2, 3, 19, 7, 3, 0, 1, 2, 0, 0, 0, 10, 1, 0, 0, 12, 4, 5, 26, 1, 0, 1, 4, 4, 1, 1, 0, 9, 1, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 3, 0, 0, 0, 1, 1, 1, 0, 5, 3, 1, 10, 0, 0, 0, 2, 2, 1, 2, 11, 1, 4, 0, 1, 7, 12, 4, 7, 2, 3, 4, 0, 3, 1, 0, 1, 1, 0, 4, 0, 6, 5, 0, 1, 9, 7, 3, 1, 1, 0, 13, 3, 2, 1, 1, 3, 8, 3, 0, 2, 1, 2, 3, 1, 34, 3, 0, 0, 0, 4, 5, 0, 3, 0, 0, 1, 1, 11, 5, 18, 2, 3, 0, 0, 5, 1, 6, 1, 1, 6, 8, 0, 2, 2, 1, 0, 33, 0, 0, 1, 4, 0, 8, 0, 2, 1, 2, 0, 2, 0, 3, 2, 0, 1, 22, 2, 0, 1, 0, 22, 1, 1, 2, 7, 7, 1, 0, 2, 27, 16, 8, 17, 2, 1, 0, 1, 3, 4, 2, 3, 15, 1, 1, 1, 0, 1, 0, 3, 0, 1, 0, 0, 1, 0, 0, 0, 2, 1, 4, 10, 15, 1, 4, 0, 0, 0, 2, 0, 6, 0, 2, 1, 1, 0, 1, 0, 2, 0, 1, 0, 1, 1, 1, 0, 0, 4, 0, 3, 5, 31, 0, 2, 1, 0, 0, 10, 1, 2, 0, 3, 3, 1, 1, 0, 0, 0, 0, 1, 1, 0, 31, 2, 0, 2, 2, 0, 4, 2, 24, 5, 0, 0, 0, 0, 21, 2, 5, 2, 1, 22, 2, 0, 0, 0, 0, 0, 2, 1, 1, 18, 5, 3, 2, 0, 1, 2, 1, 19, 0, 0, 5, 1, 2, 0, 25, 0, 1, 17, 1, 1, 2, 11, 3, 0, 2, 1, 0, 11, 0, 0, 2, 2, 0, 5, 1, 1, 40, 0, 4, 2, 0, 0, 1, 2, 1, 0, 3, 8, 2, 2, 0, 7, 5, 12, 13, 6, 2, 0, 0, 0, 1, 0, 27, 5, 2, 2, 0, 3, 1, 4, 0, 0, 0, 2, 1, 6, 1, 2, 2, 3, 2, 3, 20, 11, 2, 0, 1, 1, 0, 0, 2, 4, 0, 1, 1, 1, 5, 2, 6, 3, 9, 1, 38, 2, 9, 3, 1, 1, 10, 5, 3, 1, 3, 1, 22, 39, 0, 12, 0, 3, 4, 0, 0, 0, 8, 14, 0, 2, 2, 3, 2, 5, 1, 0, 0, 0, 0, 3, 2, 1, 0, 3, 0, 3, 0, 2, 3, 4, 9, 2, 0, 0, 29, 4, 3, 1, 0, 4, 2, 3, 2, 0, 0, 0, 2, 0, 1, 16, 0, 0, 0, 3, 0, 5, 1, 2, 2, 0, 2, 1, 1, 1, 1, 2, 64, 1, 0, 0, 0, 5, 1, 5, 1, 6, 0, 0, 17, 2, 1, 1, 1, 1, 0, 0, 1, 13, 2, 3, 0, 1, 2, 1, 1, 2, 0, 2, 0, 0, 3, 4, 2, 0, 1, 6, 21, 0, 0, 10, 6, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 4, 2, 1, 3, 8, 6, 4, 2, 3, 41, 1, 5, 3, 6, 23, 0, 8, 15, 10, 7, 1, 5, 2, 4, 4, 2, 3, 1, 2, 0, 2, 9, 6, 15, 6, 0, 3, 0, 4, 3, 3, 1, 2, 12, 2, 4, 1, 5, 5, 5, 10, 7, 10, 3, 8, 2, 0, 19, 0, 0, 6, 11, 5, 18, 1, 5, 0, 0, 10, 3, 15, 13, 1, 2, 6, 3, 2, 4, 0, 0, 5, 280, 4, 11, 3, 0, 0, 7, 5, 2, 1, 2, 4, 0, 2, 5, 0, 1, 0, 2, 12, 4, 14, 4, 1, 5, 7, 2, 10, 9, 2, 3, 5, 0, 1, 5, 23, 5, 0, 1, 2, 4, 8, 8, 2, 17, 3, 1, 6, 1, 1, 7, 18, 0, 5, 0, 100, 5, 12, 2, 2, 7, 8, 2, 9, 1, 1, 0, 1, 8, 10, 10, 0, 0, 0, 6, 3, 3, 0, 133, 14, 6, 38, 0, 9, 6, 5, 28, 6, 0, 0, 2, 10, 6, 1, 11, 3, 4, 2, 12, 12, 0, 10, 38, 12, 3, 8, 23, 6, 4, 0, 0, 10, 1, 2, 2, 9, 1, 7, 8, 7, 9, 4, 7, 4, 9, 11, 3, 1, 2, 4, 2, 6, 10, 2, 1, 3, 11, 0, 2, 9, 6, 3, 8, 1, 0, 5, 10, 0, 5, 7, 0, 12, 2, 0, 3, 13, 1, 9, 5, 8, 0, 0, 1, 7, 4, 0, 4, 1, 8, 3]\n","Average number of ADRs retrieved: 4.9533527696793005\n","('TF-IDF results: ', 'precision:', 0.053189592119717855, 'recall:', 0.06610853128495897, 'f1-score:', 0.045596254308149814, 'accuracy', 0.9995542792842118)\n"]}]},{"cell_type":"code","source":["print(get_scores('bm25'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bY6ZUVKPeK_u","executionInfo":{"status":"ok","timestamp":1680531744883,"user_tz":-60,"elapsed":279857,"user":{"displayName":"Karina","userId":"10900104624874413001"}},"outputId":"56dd8d9d-d0e9-4293-9dab-6227fa05b434"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-18-02f9f17a0e79>:32: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  scores_df['normalized_score'] = (scores_df['score'] - scores_df.score.min()) / (scores_df.score.max() - scores_df.score.min())\n","<ipython-input-18-02f9f17a0e79>:32: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  scores_df['normalized_score'] = (scores_df['score'] - scores_df.score.min()) / (scores_df.score.max() - scores_df.score.min())\n"]},{"output_type":"stream","name":"stdout","text":["Number of ADRs retrieved for each review [6, 5, 34, 4, 3, 2, 4, 2, 2, 2, 2, 1, 3, 9, 20, 2, 3, 1, 7, 2, 3, 2, 11, 1, 9, 16, 1, 9, 1, 15, 1, 8, 3, 2, 1, 0, 1, 7, 5, 12, 21, 1, 2, 10, 1, 5, 2, 10, 12, 5, 3, 4, 5, 26, 2, 2, 37, 8, 7, 19, 3, 8, 6, 1, 6, 6, 12, 2, 8, 10, 1, 3, 1, 3, 5, 5, 5, 5, 4, 1, 1, 3, 7, 2, 12, 6, 41, 5, 9, 3, 4, 2, 4, 2, 6, 2, 11, 3, 4, 2, 1, 3, 21, 6, 6, 1, 22, 6, 5, 3, 1, 6, 3, 10, 5, 9, 3, 4, 2, 4, 3, 1, 9, 2, 8, 2, 5, 4, 1, 5, 5, 2, 2, 2, 1, 10, 15, 22, 1, 1, 3, 13, 12, 5, 1, 4, 2, 3, 2, 2, 5, 2, 7, 1, 9, 2, 6, 1, 9, 2, 3, 2, 5, 2, 1, 2, 5, 3, 3, 2, 6, 4, 8, 5, 14, 1, 1, 12, 3, 6, 76, 2, 8, 2, 3, 3, 2, 6, 2, 2, 1, 11, 11, 10, 33, 2, 5, 9, 7, 28, 1, 7, 16, 4, 4, 7, 12, 2, 6, 4, 10, 6, 3, 7, 1, 2, 2, 3, 8, 3, 2, 7, 4, 1, 3, 3, 6, 2, 1, 2, 0, 2, 3, 2, 6, 3, 1, 7, 4, 4, 8, 3, 5, 4, 2, 1, 6, 3, 9, 5, 3, 5, 3, 7, 1, 10, 10, 5, 0, 3, 6, 1, 3, 6, 5, 4, 1, 24, 40, 1, 4, 1, 1, 3, 3, 4, 1, 15, 3, 2, 5, 1, 1, 5, 1, 10, 7, 3, 6, 3, 6, 6, 2, 7, 3, 2, 2, 5, 3, 1, 35, 2, 10, 3, 15, 5, 16, 5, 6, 2, 1, 1, 5, 13, 2, 1, 7, 6, 1, 4, 6, 3, 1, 1, 5, 7, 4, 12, 8, 18, 1, 5, 3, 1, 5, 21, 6, 9, 2, 1, 3, 2, 7, 5, 7, 1, 2, 2, 6, 2, 2, 2, 5, 9, 1, 9, 4, 1, 10, 6, 7, 1, 5, 3, 7, 2, 1, 8, 3, 2, 5, 3, 2, 2, 1, 0, 7, 2, 1, 4, 1, 2, 4, 3, 2, 1, 2, 11, 2, 3, 11, 1, 4, 12, 1, 4, 8, 1, 6, 4, 49, 9, 2, 8, 6, 4, 5, 3, 8, 1, 1, 5, 1, 3, 3, 5, 1, 7, 7, 5, 4, 1, 3, 3, 17, 2, 2, 10, 4, 57, 2, 1, 1, 2, 1, 9, 19, 1, 6, 1, 5, 7, 13, 3, 5, 2, 1, 9, 1, 3, 1, 3, 3, 1, 16, 4, 4, 3, 3, 6, 2, 3, 1, 1, 2, 7, 3, 4, 1, 1, 2, 1, 3, 6, 3, 17, 2, 6, 5, 5, 2, 1, 1, 10, 5, 2, 1, 3, 18, 4, 1, 4, 2, 5, 3, 4, 1, 2, 5, 2, 12, 2, 6, 59, 2, 1, 3, 9, 13, 2, 14, 1, 3, 21, 1, 2, 7, 3, 7, 12, 5, 1, 1, 4, 34, 6, 3, 7, 2, 1, 2, 6, 1, 2, 6, 1, 4, 19, 1, 6, 2, 5, 2, 1, 1, 2, 7, 1, 5, 3, 3, 5, 1, 1, 2, 5, 6, 5, 10, 6, 3, 3, 2, 4, 1, 1, 2, 1, 3, 1, 5, 3, 2, 15, 1, 2, 8, 2, 7, 4, 3, 6, 1, 3, 8, 3, 1, 2, 4, 4, 3, 5, 2, 5, 1, 8, 2, 2, 7, 7, 11, 1, 3, 7, 9, 15, 7, 2, 4, 6, 1, 10, 2, 1, 19, 8, 4, 1, 3, 3, 2, 2, 3, 2, 2, 14, 3, 4, 3, 17, 3, 5, 2, 2, 3, 3, 2, 2, 5, 13, 4, 1, 2, 1, 1, 2, 1, 7, 2, 1, 2, 2, 12, 4, 1, 1, 31, 6, 23, 3, 1, 12, 6, 6, 2, 3, 1, 2, 3, 1, 17, 6, 5, 1, 3, 1, 2, 3, 60, 8, 6, 4, 6, 5, 2, 2, 1, 3, 4, 3, 7, 7, 1, 2, 2, 1, 1, 2, 6, 4, 1, 3, 3, 11, 5, 3, 5, 6, 2, 2, 5, 7, 3, 2, 1, 5, 12, 2, 4, 1, 1, 2, 1, 3, 4, 6, 5, 3, 3, 4, 57, 35, 1, 2, 12, 1, 6, 1, 2, 9, 6, 4, 4, 6, 12, 8, 6, 3, 1, 2, 5, 18, 7, 6, 1, 6, 3, 8, 1, 1, 7, 1, 3, 9, 1, 17, 5, 5, 4, 13, 2, 2, 1, 11, 1, 4, 2, 5, 1, 1, 22, 2, 2, 8, 1, 3, 1, 13, 1, 2, 3, 2, 1, 2, 4, 2, 5, 4, 1, 2, 1, 2, 5, 3, 3, 8, 9, 7, 8, 9, 28, 11, 7, 1, 5, 8, 5, 5, 6, 4, 7, 4, 21, 1, 4, 6, 25, 3, 6, 2, 3, 3, 14, 2, 10, 1, 1, 1, 1, 7, 1, 1, 5, 11, 8, 1, 1, 5, 4, 3, 3, 1, 3, 3, 3, 7, 9, 6, 1, 13, 29, 3, 1, 5, 5, 4, 21, 3, 6, 31, 1, 4, 1, 4, 5, 6, 2, 1, 1, 5, 5, 5, 19, 3, 2, 4, 4, 31, 4, 2, 3, 2, 3, 5, 4, 9, 4, 5, 1, 21, 1, 14, 1, 4, 2, 9, 16, 1, 5, 41, 6, 5, 1, 4, 2, 3, 4, 2, 2, 1, 9, 8, 2, 11, 1, 7, 9, 4, 6, 6, 3, 1, 7, 29, 5, 3, 1, 7, 5, 4, 1, 5, 5, 3, 2, 1, 4, 12, 1, 1, 4, 3, 1, 5, 3, 15, 1, 7, 4, 7, 1, 2, 9, 1, 4, 4, 2, 1, 9, 9, 6, 1, 3, 15, 1, 19, 5, 2, 4, 3, 6, 3, 3, 5, 2, 1, 6, 5, 3, 6, 1, 5, 5, 1, 1, 6, 1, 3, 3, 5, 3, 3, 4, 29, 5, 26, 4, 1, 9, 4, 3, 15, 12, 2, 5, 57, 3, 3, 2, 4, 7, 4, 2, 8, 4, 1, 1, 4, 3, 9, 3, 2, 2, 2, 4, 6, 6, 1, 5, 3, 1, 21, 1, 14, 2, 3, 6, 1, 3, 2, 1, 11, 16, 27, 1, 30, 2, 3, 3, 9, 2, 1, 6, 5, 10, 4, 3, 2, 3, 7, 11, 2, 1, 1, 5, 12, 17, 4, 3, 14, 1, 2, 1, 14, 1, 4, 2, 10, 16, 2, 9, 12, 7, 4, 4, 23, 1, 1, 8, 3, 1, 1, 18, 2, 3, 3, 1, 1, 57, 3, 4, 2, 5, 2, 2, 1, 4, 3, 6, 3, 2, 4, 4, 3, 1, 2, 4, 1, 5, 3, 14, 36, 2, 1, 5, 1, 4, 10, 9, 2, 3, 5, 2, 3, 2, 6, 3, 3, 3, 1, 2, 3, 2, 6, 3, 2, 7, 2, 1, 2, 2, 3, 4, 6, 1, 4, 3, 8, 5, 6, 1, 1, 1, 7, 2, 6, 2, 3, 28, 3, 7, 5, 2, 1, 3, 2, 2, 1, 6, 3, 6, 42, 1, 1, 1, 4, 5, 5, 2, 5, 10, 3, 6, 4, 9, 8, 6, 6, 4, 1, 7, 9, 1, 1, 3, 3, 2, 2, 7, 1, 2, 4, 4, 1, 1, 2, 2, 1, 3, 1, 2, 3, 5, 2, 1, 2, 5, 2, 1, 3, 1, 1, 1, 1, 8, 2, 2, 1, 4, 8, 4, 2, 1, 6, 6, 1, 2, 7, 3, 2, 3, 5, 2, 4, 5, 4, 3, 3, 3, 1, 4, 4, 1, 6, 5, 1, 1, 3, 1, 7, 6, 2, 1, 3, 2, 1, 1, 2, 4, 2, 1, 1, 11, 21, 4, 9, 1, 6, 2, 1, 2, 6, 5, 5, 6, 3, 1, 2, 18, 1, 18, 4, 2, 18, 1, 2, 5, 6, 1, 3, 1, 8, 3, 1, 2, 4, 2, 13, 12, 2, 10, 23, 13, 2, 1, 12, 2, 1, 12, 3, 2, 1, 3, 1, 6, 1, 7, 1, 6, 2, 1, 1, 2, 3, 3, 3, 1, 3, 2, 4, 1, 2, 8, 4, 2, 8, 1, 2, 4, 3, 5, 3, 3, 5, 4, 2, 6, 2, 2, 1, 6, 4, 1, 6, 8, 1, 1, 1, 2, 2, 3, 2, 7, 1, 3, 11, 6, 8, 5, 7, 7, 1, 3, 4, 3, 3, 2, 5, 2, 1, 2, 1, 4, 13, 4, 1, 15, 1, 3, 4, 5, 3, 22, 2, 3, 1, 19, 10, 1, 4, 1, 1, 1, 5, 2, 7, 2, 4, 6, 1, 4, 7, 6, 3, 3, 7, 1, 3, 5, 3, 5, 2, 5, 1, 9, 5, 7, 1, 2, 7, 1, 2, 1, 2, 10, 2, 1, 30, 1, 7, 2, 4, 3, 24, 3, 1, 3, 25, 6, 1, 1, 3, 2, 8, 6, 4, 3, 2, 4, 5, 2, 2, 1, 5, 5, 1, 1, 3, 2, 2, 1, 3, 7, 3, 2, 5, 6, 1, 3, 1, 13, 6, 4, 2, 1, 2, 1, 2, 1, 1, 3, 2, 25, 2, 8, 3, 3, 4, 2, 4, 3, 5, 1, 1, 8, 1, 3, 3, 6, 5, 1, 7, 13, 4, 3, 2, 29, 4, 1, 8, 5, 4, 2, 1, 1, 2, 1, 6, 6, 4, 3, 2, 6, 3, 8, 5, 2, 1, 1, 1, 39, 8, 4, 1, 7, 4, 3, 5, 1, 7, 1, 2, 1, 4, 5, 2, 4, 11, 3, 5, 7, 4, 1, 4, 3, 13, 7, 2, 1, 3, 1, 1, 3, 1, 2, 8, 4, 1, 1, 6, 3, 8, 6, 6, 19, 3, 6, 3, 3, 1, 1, 3, 6, 5, 6, 11, 4, 4, 7, 3, 5, 5, 1, 5, 7, 3, 3, 4, 1, 13, 12, 13, 2, 2, 5, 2, 12, 2, 4, 3, 7, 4, 1, 7, 2, 1, 5, 10, 4, 1, 2, 1, 12, 2, 2, 1, 5, 13, 3, 13, 4, 3, 1, 1, 4, 3, 1, 2, 1, 5, 22, 4, 2, 3, 7, 2, 4, 2, 1, 3, 15, 2, 4, 2, 5, 10, 3, 3, 1, 1, 4, 2, 3, 4, 7, 12, 8, 4, 6, 2, 6, 2, 1, 2, 4, 2, 1, 4, 14, 3, 5, 2, 1, 5, 2, 5, 2, 3, 6, 16, 1, 2, 4, 6, 1, 8, 1, 1, 3, 5, 5, 3, 2, 6, 2, 21, 2, 4, 2, 1, 1, 2, 3, 3, 1, 1, 21, 6, 5, 3, 3, 7, 3, 8, 2, 2, 7, 10, 12, 5, 1, 4, 1, 3, 2, 2, 2, 4, 10, 9, 3, 8, 1, 3, 2, 4, 2, 3, 1, 7, 9, 13, 2, 4, 1, 10, 2, 6, 5, 6, 13, 32, 3, 1, 1, 4, 1, 1, 1, 3, 13, 1, 15, 1, 8, 2, 2, 2, 6, 17, 5, 4, 25, 3, 24, 5, 2, 3, 2, 8, 2, 6, 1, 8, 1, 6, 3, 1, 4, 3, 2, 3, 1, 2, 23, 5, 1, 1, 2, 2, 20, 7, 4, 4, 2, 6, 1, 10, 4, 9, 2, 3, 7, 6, 12, 4, 4, 3, 2, 8, 4, 16, 3, 1, 2, 6, 2, 1, 3, 7, 6, 2, 1, 4, 6, 2, 4, 9, 1, 7, 6, 3, 3, 6, 3, 3, 2, 10, 13, 9, 12, 3, 4, 3, 8, 6, 2, 8, 7, 3, 5, 11, 3, 5, 1, 7, 7, 5, 3, 6, 9, 4, 9, 11, 2, 17, 13, 10, 2, 5, 1, 1, 3, 5, 2, 2, 2, 3, 9, 12, 5, 6, 1, 4, 7, 7, 7, 24, 10, 5, 2, 2, 6, 11, 3, 5, 6, 3, 3, 9, 2, 4, 5, 7, 1, 3, 1, 20, 7, 4, 7, 11, 7, 3, 8, 2, 9, 2, 9, 6, 2, 25, 6, 9, 1, 3, 7, 13, 1, 3, 2, 6, 3, 3, 3, 9, 9, 3, 4, 9, 1, 7, 3, 5, 8, 6, 3, 8, 7, 6, 4, 1, 4, 4, 5, 1, 12, 7, 7, 3, 8, 5, 6, 2, 6, 3, 7, 4, 16, 6, 4, 27, 3, 5, 2, 11, 5, 3, 1, 3, 4, 4, 6, 5, 3, 8, 5, 13, 5, 1, 2, 2, 3, 5, 4, 8, 1, 3]\n","Average number of ADRs retrieved: 5.181729834791059\n","('BM25 results: ', 'precision:', 0.1243069558805882, 'recall:', 0.09294909339820155, 'f1-score:', 0.09025182054916552, 'accuracy', 0.999561940884928)\n"]}]},{"cell_type":"code","source":["# pass the model you want to use ('tf_idf' or 'bm25') and a review. The function outputs the ADRs it retrieves for the review you inputed."],"metadata":{"id":"E0Dd63vrkXYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_ADRs_for_new_review(model, review, lexicon_list=lexicon_list, threshold_bm25=0.78, threshold_tf_idf=0.32):\n","\n","    preprocessed_ADRs = [preprocessing(i, remove_sw=True) for i in lexicon_list]\n","    tokenized_review = preprocessing(review, remove_sw=True)\n","\n","    # choose model\n","    if model == 'bm25':\n","        bm25 = BM25Okapi(preprocessed_ADRs)\n","\n","        # get the scores for every ADRs for this specific review\n","        score_list = bm25.get_scores(tokenized_review)\n","\n","        # build dataframe with scores\n","        scores_df = pd.DataFrame({'ADR': lexicon_list, 'score': score_list})\n","\n","        # remove rows with score=0\n","        scores_df = scores_df[scores_df['score'] != 0]\n","\n","        # normalize scores\n","        scores_df['normalized_score'] = (scores_df['score'] - scores_df.score.min()) / (scores_df.score.max() - scores_df.score.min())\n","\n","        # keep only scores over thershold\n","        scores_df = scores_df[scores_df['normalized_score'] > threshold_bm25]\n","\n","        # get final list of ADRs obtained from model\n","        model_ADRs = scores_df.ADR.to_list()\n","\n","    elif model == 'tf_idf':\n","        vectorizer = TfidfVectorizer()\n","        tfidf_matrix = vectorizer.fit_transform(lexicon_list)\n","\n","        query_tfidf = vectorizer.transform([review])\n","\n","        # get scores\n","        cosine_similarities = cosine_similarity(query_tfidf, tfidf_matrix).flatten()\n","\n","        # build dataframe with scores\n","        scores_df = pd.DataFrame({'ADR': lexicon_list, 'score': cosine_similarities})\n","\n","        # keep only scores over thershold\n","        scores_df = scores_df[scores_df['score'] > threshold_tf_idf]\n","\n","        # get final list of ADRs obtained from model\n","        model_ADRs = scores_df.ADR.to_list()\n","\n","    return model_ADRs"],"metadata":{"id":"gs8yLnbkf8Gb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(get_ADRs_for_new_review('tf_idf','i was suffering from insomnia before starting taking some medicine. I did not feel any dizziness in the morning as reported in the leaflet, but sometimes i have been feeling nauseous when i wake up.'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WHkOvTOMiaZZ","executionInfo":{"status":"ok","timestamp":1680564112869,"user_tz":-60,"elapsed":2016,"user":{"displayName":"Karina","userId":"10900104624874413001"}},"outputId":"36ca4dfb-86f5-4208-a8e1-b1cca8d7a937"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['groggy in the morning']\n"]}]},{"cell_type":"code","source":["print(get_ADRs_for_new_review('bm25', 'i was suffering from insomnia before starting taking some medicine. I did not feel any dizziness in the morning as reported in the leaflet, but sometimes i have been feeling nauseous when i wake up.'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P-dSCQawj7C6","executionInfo":{"status":"ok","timestamp":1680564395643,"user_tz":-60,"elapsed":551,"user":{"displayName":"Karina","userId":"10900104624874413001"}},"outputId":"d0debad0-61cd-4fa7-c8cc-8f550931556a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['nauseous']\n"]}]}]}